# Read the CSV file and assign it to the variable "comm_data"
comm_data <- read.csv("CommQuest2023_Larger.csv")
comm_data
knitr::opts_chunk$set(echo = TRUE)
# Load Tidyverse
library(tidyverse)
# Read the CSV file and assign it to the variable "comm_data"
comm_data <- read.csv("CommQuest2023_Larger.csv")
comm_data
# Create a new dataframe with selected columns
new_df1<- comm_data%>% select(date,channel,message)
# Overview of new dataframe
glimpse(new_df1)
#Arrange the dataframe in ascending order based on the "date" column
arranged_data <- comm_data %>%arrange(date)
print(arranged_data)
#Use distinct() to find unique senders
unique_senders <- comm_data %>%distinct(sender)%>%select(sender)
print(unique_senders)
# Group by sender and count messages
summary_table <- comm_data %>%group_by(sender) %>%summarise(count = n())
print(summary_table)
#Another method just using count()
##summary_table2 <- comm_data %>%group_by(sender)%>%count(sender)
#Break down of each message and count
##summary_table2 <- comm_data %>%group_by(sender)%>%*count(message)*
# Group by channel and count messages
summary_table2 <- comm_data %>%group_by(channel) %>%summarise(count = n())
print(summary_table2)
#Another method
##summary_table2 <- comm_data %>%group_by(channel) %>%count(channel)
# Filter for positive sentiment scores
positive_data <- comm_data %>%
filter(sentiment> 0)
# Group by sender and calculate the average sentiment score
sender_avg_sentiment <- positive_data %>%
group_by(sender) %>%
summarise(avg_sentiment=mean(sentiment))
# Arrange senders in descending order of average sentiment score
top_senders <- sender_avg_sentiment %>%
arrange(desc(avg_sentiment))
# Select the top three senders
top_three_senders <- top_senders %>%
slice(1:3)
# Display the top three senders and their corresponding sentiment averages
print(top_three_senders)
# Group by date and calculate the average sentiment score for each day
daily_avg_sentiment <- comm_data %>%
group_by(date) %>%
summarise(avg_sentiment = mean(sentiment))
# Arrange the results in ascending order of date
daily_avg_sentiment <- daily_avg_sentiment %>%
arrange(date)
# Display the dataframe with daily average sentiment scores
print(daily_avg_sentiment)
#another method
##daily<-comm_data%>%group_by(date)%>%summarise(mean(sentiment))%>%arrange(date)
# Mutate to add a new column "sentiment_label" based on sentiment scores
comm_data<-comm_data %>% select(sentiment) %>% mutate(positive = as.logical(sentiment > 0), neutral = as.logical(sentiment == 0), negative = as.logical(sentiment < 0))
print(comm_data)
comm_data %>% select(sentiment) %>% mutate(sentiment_label = ifelse(as.logical(sentiment > 0), "Positive", ifelse(as.logical(sentiment < 0), "Negative", "Neutral")))
print(comm_data)
# Mutate to calculate the product of sentiment score and message length
new_dataframe <- comm_data %>%select(message,sentiment)%>%mutate(product_score_length = sentiment*nchar(message))
knitr::opts_chunk$set(echo = TRUE)
# Load Tidyverse
library(tidyverse)
# Read the CSV file and assign it to the variable "comm_data"
comm_data <- read.csv("CommQuest2023_Larger.csv")
comm_data
# Create a new dataframe with selected columns
new_df1<- comm_data%>% select(date,channel,message)
# Overview of new dataframe
glimpse(new_df1)
#Arrange the dataframe in ascending order based on the "date" column
arranged_data <- comm_data %>%arrange(date)
print(arranged_data)
#Use distinct() to find unique senders
unique_senders <- comm_data %>%distinct(sender)%>%select(sender)
print(unique_senders)
# Group by sender and count messages
summary_table <- comm_data %>%group_by(sender) %>%summarise(count = n())
print(summary_table)
#Another method just using count()
##summary_table2 <- comm_data %>%group_by(sender)%>%count(sender)
#Break down of each message and count
##summary_table2 <- comm_data %>%group_by(sender)%>%*count(message)*
# Group by channel and count messages
summary_table2 <- comm_data %>%group_by(channel) %>%summarise(count = n())
print(summary_table2)
#Another method
##summary_table2 <- comm_data %>%group_by(channel) %>%count(channel)
# Filter for positive sentiment scores
positive_data <- comm_data %>%
filter(sentiment> 0)
# Group by sender and calculate the average sentiment score
sender_avg_sentiment <- positive_data %>%
group_by(sender) %>%
summarise(avg_sentiment=mean(sentiment))
# Arrange senders in descending order of average sentiment score
top_senders <- sender_avg_sentiment %>%
arrange(desc(avg_sentiment))
# Select the top three senders
top_three_senders <- top_senders %>%
slice(1:3)
# Display the top three senders and their corresponding sentiment averages
print(top_three_senders)
# Group by date and calculate the average sentiment score for each day
daily_avg_sentiment <- comm_data %>%
group_by(date) %>%
summarise(avg_sentiment = mean(sentiment))
# Arrange the results in ascending order of date
daily_avg_sentiment <- daily_avg_sentiment %>%
arrange(date)
# Display the dataframe with daily average sentiment scores
print(daily_avg_sentiment)
#another method
##daily<-comm_data%>%group_by(date)%>%summarise(mean(sentiment))%>%arrange(date)
# Mutate to add a new column "sentiment_label" based on sentiment scores
comm_data<-comm_data %>% select(sentiment) %>% mutate(positive = as.logical(sentiment > 0), neutral = as.logical(sentiment == 0), negative = as.logical(sentiment < 0))
print(comm_data)
comm_data %>% select(sentiment) %>% mutate(sentiment_label = ifelse(as.logical(sentiment > 0), "Positive", ifelse(as.logical(sentiment < 0), "Negative", "Neutral")))
print(comm_data)
# Mutate to calculate the product of sentiment score and message length
new_dataframe <- comm_data %>%select(message,sentiment)%>%mutate(product_score_length = sentiment*nchar(message))
# Mutate to calculate the product of sentiment score and message length
comm_data %>% mutate(product_score_length = sentiment*nchar(message)) %>% arrange(desc(product_score_length))
# Mutate to calculate the product of sentiment score and message length
comm_data %>% mutate(a = sentiment*nchar(message)) %>% arrange(desc(a))
# Mutate to calculate the product of sentiment score and message length
comm_data %>% mutate(a = sentiment*nchar(message))
# Mutate to calculate the product of sentiment score and message length
message
comm_data %>% mutate(a = sentiment*nchar(message)) %>% arrange(desc(a))
# Mutate to calculate the product of sentiment score and message length
mutate(a = sentiment*nchar(message)) %>% arrange(desc(a))
# Mutate to calculate the product of sentiment score and message length
mutate(a = nchar(message)) %>% arrange(desc(a))
# Mutate to calculate the product of sentiment score and message length
mutate(a = nchar(message)) %>% arrange(desc(a))
# Mutate to calculate the product of sentiment score and message length
mutate(a = nchar(message)) %>% arrange(desc(a))
knitr::opts_chunk$set(echo = TRUE)
# Load Tidyverse
library(tidyverse)
# Read the CSV file and assign it to the variable "comm_data"
comm_data <- read.csv("CommQuest2023_Larger.csv")
comm_data
# Create a new dataframe with selected columns
new_df1<- comm_data%>% select(date,channel,message)
# Overview of new dataframe
glimpse(new_df1)
#Arrange the dataframe in ascending order based on the "date" column
arranged_data <- comm_data %>%arrange(date)
print(arranged_data)
#Use distinct() to find unique senders
unique_senders <- comm_data %>%distinct(sender)%>%select(sender)
print(unique_senders)
# Group by sender and count messages
summary_table <- comm_data %>%group_by(sender) %>%summarise(count = n())
print(summary_table)
#Another method just using count()
##summary_table2 <- comm_data %>%group_by(sender)%>%count(sender)
#Break down of each message and count
##summary_table2 <- comm_data %>%group_by(sender)%>%*count(message)*
# Group by channel and count messages
summary_table2 <- comm_data %>%group_by(channel) %>%summarise(count = n())
print(summary_table2)
#Another method
##summary_table2 <- comm_data %>%group_by(channel) %>%count(channel)
# Filter for positive sentiment scores
positive_data <- comm_data %>%
filter(sentiment> 0)
# Group by sender and calculate the average sentiment score
sender_avg_sentiment <- positive_data %>%
group_by(sender) %>%
summarise(avg_sentiment=mean(sentiment))
# Arrange senders in descending order of average sentiment score
top_senders <- sender_avg_sentiment %>%
arrange(desc(avg_sentiment))
# Select the top three senders
top_three_senders <- top_senders %>%
slice(1:3)
# Display the top three senders and their corresponding sentiment averages
print(top_three_senders)
# Group by date and calculate the average sentiment score for each day
daily_avg_sentiment <- comm_data %>%
group_by(date) %>%
summarise(avg_sentiment = mean(sentiment))
# Arrange the results in ascending order of date
daily_avg_sentiment <- daily_avg_sentiment %>%
arrange(date)
# Display the dataframe with daily average sentiment scores
print(daily_avg_sentiment)
#another method
##daily<-comm_data%>%group_by(date)%>%summarise(mean(sentiment))%>%arrange(date)
# Mutate to add a new column "sentiment_label" based on sentiment scores
comm_data<-comm_data %>% select(sentiment) %>% mutate(positive = as.logical(sentiment > 0), neutral = as.logical(sentiment == 0), negative = as.logical(sentiment < 0))
print(comm_data)
comm_data %>% select(sentiment) %>% mutate(sentiment_label = ifelse(as.logical(sentiment > 0), "Positive", ifelse(as.logical(sentiment < 0), "Negative", "Neutral")))
print(comm_data)
knitr::opts_chunk$set(echo = TRUE)
# Load Tidyverse
library(tidyverse)
# Read the CSV file and assign it to the variable "comm_data"
comm_data <- read.csv("CommQuest2023_Larger.csv")
comm_data
# Create a new dataframe with selected columns
new_df1<- comm_data%>% select(date,channel,message)
# Overview of new dataframe
glimpse(new_df1)
#Arrange the dataframe in ascending order based on the "date" column
arranged_data <- comm_data %>%arrange(date)
print(arranged_data)
#Use distinct() to find unique senders
unique_senders <- comm_data %>%distinct(sender)%>%select(sender)
print(unique_senders)
# Group by sender and count messages
summary_table <- comm_data %>%group_by(sender) %>%summarise(count = n())
print(summary_table)
#Another method just using count()
##summary_table2 <- comm_data %>%group_by(sender)%>%count(sender)
#Break down of each message and count
##summary_table2 <- comm_data %>%group_by(sender)%>%*count(message)*
# Group by channel and count messages
summary_table2 <- comm_data %>%group_by(channel) %>%summarise(count = n())
print(summary_table2)
#Another method
##summary_table2 <- comm_data %>%group_by(channel) %>%count(channel)
# Filter for positive sentiment scores
positive_data <- comm_data %>%
filter(sentiment> 0)
# Group by sender and calculate the average sentiment score
sender_avg_sentiment <- positive_data %>%
group_by(sender) %>%
summarise(avg_sentiment=mean(sentiment))
# Arrange senders in descending order of average sentiment score
top_senders <- sender_avg_sentiment %>%
arrange(desc(avg_sentiment))
# Select the top three senders
top_three_senders <- top_senders %>%
slice(1:3)
# Display the top three senders and their corresponding sentiment averages
print(top_three_senders)
# Group by date and calculate the average sentiment score for each day
daily_avg_sentiment <- comm_data %>%
group_by(date) %>%
summarise(avg_sentiment = mean(sentiment))
# Arrange the results in ascending order of date
daily_avg_sentiment <- daily_avg_sentiment %>%
arrange(date)
# Display the dataframe with daily average sentiment scores
print(daily_avg_sentiment)
#another method
##daily<-comm_data%>%group_by(date)%>%summarise(mean(sentiment))%>%arrange(date)
# Mutate to add a new column "sentiment_label" based on sentiment scores
comm_data<-comm_data %>% select(sentiment) %>% mutate(positive = as.logical(sentiment > 0), neutral = as.logical(sentiment == 0), negative = as.logical(sentiment < 0))
print(comm_data)
comm_data %>% select(sentiment) %>% mutate(sentiment_label = ifelse(as.logical(sentiment > 0), "Positive", ifelse(as.logical(sentiment < 0), "Negative", "Neutral")))
print(comm_data)
#SHINY ERROR, INTRA ERROR, UNABLE TO RESOLVE
comm_data %>% mutate(a = sentiment*nchar(message()))%>%arrange(desc(a))
#SHINY ERROR, INTRA ERROR, UNABLE TO RESOLVE
comm_data %>% mutate(a = nchar(message()))%>%arrange(desc(a))
#SHINY ERROR, INTRA ERROR, UNABLE TO RESOLVE
message()
#comm_data %>% mutate(a = sentiment*nchar(message()))%>%arrange(desc(a))
#SHINY ERROR, INTRA ERROR, UNABLE TO RESOLVE
#comm_data %>% mutate(a = sentiment*nchar(message%>%arrange(desc(a))
message
#SHINY ERROR, INTRA ERROR, UNABLE TO RESOLVE
#comm_data %>% mutate(a = sentiment*nchar(message%>%arrange(desc(a))
comm_data
#SHINY ERROR, INTRA ERROR, UNABLE TO RESOLVE
comm_data_org %>% mutate(a = sentiment*nchar(message%>%arrange(desc(a))
knitr::opts_chunk$set(echo = TRUE)
# Load Tidyverse
library(tidyverse)
# Read the CSV file and assign it to the variable "comm_data"
comm_data <- read.csv("CommQuest2023_Larger.csv")
comm_data_org <- read.csv("CommQuest2023_Larger.csv")
# Create a new dataframe with selected columns
new_df1<- comm_data%>% select(date,channel,message)
# Overview of new dataframe
glimpse(new_df1)
#Arrange the dataframe in ascending order based on the "date" column
arranged_data <- comm_data %>%arrange(date)
print(arranged_data)
#Use distinct() to find unique senders
unique_senders <- comm_data %>%distinct(sender)%>%select(sender)
print(unique_senders)
# Group by sender and count messages
summary_table <- comm_data %>%group_by(sender) %>%summarise(count = n())
print(summary_table)
#Another method just using count()
##summary_table2 <- comm_data %>%group_by(sender)%>%count(sender)
#Break down of each message and count
##summary_table2 <- comm_data %>%group_by(sender)%>%*count(message)*
# Group by channel and count messages
summary_table2 <- comm_data %>%group_by(channel) %>%summarise(count = n())
print(summary_table2)
#Another method
##summary_table2 <- comm_data %>%group_by(channel) %>%count(channel)
# Filter for positive sentiment scores
positive_data <- comm_data %>%
filter(sentiment> 0)
# Group by sender and calculate the average sentiment score
sender_avg_sentiment <- positive_data %>%
group_by(sender) %>%
summarise(avg_sentiment=mean(sentiment))
# Arrange senders in descending order of average sentiment score
top_senders <- sender_avg_sentiment %>%
arrange(desc(avg_sentiment))
# Select the top three senders
top_three_senders <- top_senders %>%
slice(1:3)
# Display the top three senders and their corresponding sentiment averages
print(top_three_senders)
# Group by date and calculate the average sentiment score for each day
daily_avg_sentiment <- comm_data %>%
group_by(date) %>%
summarise(avg_sentiment = mean(sentiment))
# Arrange the results in ascending order of date
daily_avg_sentiment <- daily_avg_sentiment %>%
arrange(date)
# Display the dataframe with daily average sentiment scores
print(daily_avg_sentiment)
#another method
##daily<-comm_data%>%group_by(date)%>%summarise(mean(sentiment))%>%arrange(date)
# Mutate to add a new column "sentiment_label" based on sentiment scores
comm_data<-comm_data %>% select(sentiment) %>% mutate(positive = as.logical(sentiment > 0), neutral = as.logical(sentiment == 0), negative = as.logical(sentiment < 0))
print(comm_data)
comm_data %>% select(sentiment) %>% mutate(sentiment_label = ifelse(as.logical(sentiment > 0), "Positive", ifelse(as.logical(sentiment < 0), "Negative", "Neutral")))
print(comm_data)
#SHINY ERROR, INTRA ERROR, UNABLE TO RESOLVE
comm_data_org %>% mutate(a = sentiment*nchar(message%>%arrange(desc(a))
#SHINY ERROR, INTRA ERROR, UNABLE TO RESOLVE
comm_data %>% mutate(a = sentiment*nchar(comm_data_org$message%>%arrange(desc(a))
#SHINY ERROR, INTRA ERROR, UNABLE TO RESOLVE
comm_data %>% mutate(a = sentiment*nchar(comm_data_org$message)%>%arrange(desc(a))
#SHINY ERROR, INTRA ERROR, UNABLE TO RESOLVE
comm_data %>% mutate(a = sentiment*nchar(comm_data_org$message))%>%arrange(desc(a))
install.packages("install.packages("gptstudio")")
install.packages("gptstudio")
# install.packages("pak")
pak::pak("MichelNivard/gptstudio")
# install.packages("pak")
pak::pak("MichelNivard/gptstudio")
pak::pak("MichelNivard/gptstudio")
gptstudio:::addin_chatgpt()
require(usethis)
edit_r_environ()
# install.packages("pak")
pak::pak("JamesHWade/gpttools")
# install.packages("pak")
pak::pak("JamesHWade/gpttools")
# Enable repository from jameshwade
options(repos = c(
jameshwade = "https://jameshwade.r-universe.dev",
CRAN = "https://cloud.r-project.org"
))
# Download and install gpttools in R
install.packages("gpttools")
# Browse the gpttools manual pages
help(package = "gpttools")
require(devtools)
install.packages(devtools)
c
install.packages(c)
require(usethis)
edit_r_environ()
Sys.setenv(OPENAI_API_KEY = "<C>")
Sys.setenv(OPENAI_API_KEY = "<c>")
Sys.setenv(OPENAI_API_KEY = "c")
OPENAI_API_KEY="<C>"
OPENAI_API_KEY="<c>"
OPENAI_API_KEY="c"
Sys.setenv(OPENAI_API_KEY = "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")
OPENAI_API_KEY="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"
Sys.setenv(OPENAI_API_KEY = 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)
require(devtools)
install.packages("gptstudio")
install.packages("gptstudio")
# install.packages("pak")
pak::pak("MichelNivard/gptstudio")
# install.packages("MichelNivard/gptstudio")
install.packages("MichelNivard/gptstudio")
require(usethis)
edit_r_environ()
gptstudio:::addin_chatgpt()
{OPENAI_API_KEY}
{OPENAI_API_KEY}
require(devtools)
Sys.setenv(OPENAI_API_KEY = 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)
Sys.setenv(OPENAI_API_KEY = Sys.setenv(OPENAI_API_KEY = "<APIKEY>")
Sys.setenv(OPENAI_API_KEY = Sys.setenv(OPENAI_API_KEY = "<APIKEY>")
Sys.setenv(OPENAI_API_KEY = "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")
require(devtools)
# Install devtools from CRAN
install.packages("devtools")
# Or the development version from GitHub:
# install.packages("devtools")
devtools::install_github("r-lib/devtools")
require(devtools)
install_github("michelNivard/gptstudio")
openai
OPENAI_API_KEY
install.packages("chatgpt")
install(chatgptstudio)
chatgpt:::run_addin_ask_chatgpt()
gptstudio:::addin_chatgpt()
{OPENAI}
require(usethis)
require(usethis)
edit_r_environ(scope = "project")
# Install devtools from CRAN
install.packages("devtools")
# Or the development version from GitHub:
# install.packages("devtools")
devtools::install_github("r-lib/devtools")
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
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
openai_api_key=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
"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"
library(gptstudio)
write from prompt
prompt()
prompt(Can you list at least two reasons why the dataset illustrated in slide 10 is non-tidy? How can it be made Tidy?
uninstall.package
install.packages("pak")
pak::pak("usethis")
pak::pak("michelnivard/gptstudio")
usethi
usethis::edit_r_environ()
Sys.getenv("OPENAI_API_KEY")
Sys.getenv("OPENAI_API_KEY")
OPENAI_API_KEY=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
OPENAI_API_KEY="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"
Sys.getenv("OPENAI_API_KEY")
Sys.getenv("OPENAI_API_KEY=Sys.getenv("OPENAI_API_KEY")")
Sys.getenv("OPENAI_API_KEY=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")
install.packages("gptstudio")
install.packages("gptstudio")
install.packages("openai")
library(openai)
Sys.setenv(OPENAI_API_KEY="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")
install.packages("devtools")
require(devtools)
install_github("MichelNivard/gptstudio,force=TRUE")
install_github("MichelNivard/gptstudio",force=TRUE)
Sys.setenv(OPENAI_API_KEY="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")
gptstudio:::addin_chatgpt()
#Use the group_by, summarise, and arrange commands to find the day with the highest total number of characters sent across all messages in the "comm_data" dataframe.
#Use the group_by, summarise, and arrange commands to find the day with the highest total number of characters sent across all messages in the "comm_data" dataframe.
#Use the group_by, summarise, and arrange commands to find the day with the highest total number of characters sent across all messages in the "comm_data" dataframe.
#Use the group_by, summarise, and arrange commands to find the day with the highest total number of characters sent across all messages in the "comm_data" dataframe.
#Use the group_by, summarise, and arrange commands to find the day with the highest total number of characters sent across all messages in the "comm_data" dataframe.
#Use the group_by, summarise, and arrange commands to find the day with the highest total number of characters sent across all messages in the "comm_data" dataframe.
#Use the group_by, summarise, and arrange commands to find the day with the highest total number of characters sent across all messages in the "comm_data" dataframe.
#| echo: false
2 * 2
#| echo: false
2 * 2
#| echo: false
2 * 2
#| echo: false
2 * 2
Sys.setenv(OPENAI_API_KEY="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")
> gptstudio:::addin_chatgpt()
Sys.setenv(OPENAI_API_KEY="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")
gptstudio:::addin_chatgpt()
install.packages("gptstudio")
install.packages("gptstudio")
# install.packages("pak")
pak::pak("MichelNivard/gptstudio")
install.packages("httr")
install.packages("jsonlite")
C
api_key <- "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"
chat_with_gpt <- function(prompt) {
endpoint <- "https://api.openai.com/v1/engines/davinci-codex/completions"
headers <- list(
"Content-Type" = "application/json",
"Authorization" = paste("Bearer", api_key)
)
data <- list(
prompt = prompt,
max_tokens = 150  # Adjust this based on your needs
)
response <- httr::POST(endpoint, httr::add_headers(.headers=headers), body = jsonlite::toJSON(data))
result <- jsonlite::fromJSON(httr::content(response, "text"))
return(result$text)
}
prompt <- "Generate R code to read a CSV file and plot a histogram of a column."
generated_code <- chat_with_gpt(prompt)
prompt <- "Generate R code to read a CSV file and plot a histogram of a column."
generated_code <- chat_with_gpt(prompt)
prompt <- "Generate R code to read a CSV file and plot a histogram of a column."
generated_code <- chat_with_gpt(prompt)
cat(generated_code)
install.packages("gptstudio")
install.packages("gptstudio")
# install.packages("pak")
pak::pak("MichelNivard/gptstudio")
require(usethis)
edit_r_environ()
gptstudio:::addin_chatgpt()
rstudioapi::viewer("http://127.0.0.1:5607")
query(Employ the count and group_by commands to generate a summary table that shows the count of messages sent by each sender in the "comm_data" dataframe.)
query<-Employ the count and group_by commands to generate a summary table that shows the count of messages sent by each sender in the "comm_data" dataframe.
query<- "Employ the count and group_by commands to generate a summary table that shows the count of messages sent by each sender in the "comm_data" dataframe."
setwd("~/NM2207/WEEK 5")
